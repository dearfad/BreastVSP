import streamlit as st
from libs.models.mscope import get_model
from libs.info.vsp import get_patient_info
import speech_recognition as sr
# from libs.asr.speechrecognition import listen
from libs.tts.edge import say

st.set_page_config(
    page_title="ä¹³è…ºå¤–ç§‘è™šæ‹Ÿç—…äºº",
    page_icon="ğŸ‘©",
    layout='wide',
    initial_sidebar_state='expanded',
)

st.echo('testsdaf')

if "patient" not in st.session_state:
    st.session_state.patient = get_patient_info()

if "messages" not in st.session_state:
    st.session_state.messages = []


with st.sidebar:
    st.title('ğŸ‘© - BreastVSP -')
    st.image('patient.jpeg')
    st.markdown(f'''
            #### {st.session_state.patient.name}
            ###### å¹´é¾„ï¼š{st.session_state.patient.age}å²
            ###### åœ°å€ï¼š{st.session_state.patient.address}
            ###### ç”µè¯ï¼š{st.session_state.patient.phone}
            ''')
    if st.button('æ›´æ”¹æ‚£è€…', use_container_width=True):
        st.session_state.patient = get_patient_info()

    with st.expander('ğŸ“‚ è®¾ç½®'):
        with st.container(border=True):
            llm = st.toggle('å¤§è¯­è¨€æ¨¡å‹')
            model = st.selectbox('è¯­è¨€æ¨¡å‹', ['ZhipuAI/chatglm3-6b'])
            if llm:
                tokenizer, model = get_model()
        with st.container(border=True):
            micphone = st.toggle('éº¦å…‹é£è¾“å…¥')
            mic_placeholder = st.empty()
            asr = st.selectbox('è¯­éŸ³è¯†åˆ«', ['openai/whisper'])
        with st.container(border=True):
            voice = st.toggle('è¯­éŸ³è¾“å‡º')
            tts = st.selectbox('å‘éŸ³æ¨¡å‹', ['rany2/edge-tts'])
            tts_voice = st.selectbox('è¯­éŸ³é€‰æ‹©', ['zh-CN-XiaoxiaoNeural', 'zh-CN-XiaoyiNeural', 'zh-CN-liaoning-XiaobeiNeural',
                                              'zh-CN-shaanxi-XiaoniNeural', 'zh-HK-HiuGaaiNeural', 'zh-HK-HiuMaanNeural', 'zh-TW-HsiaoChenNeural', 'zh-TW-HsiaoYuNeural'])
        with st.container(border=True):
            patient = st.selectbox('æ‚£è€…ä¿¡æ¯', ['random'])

        if st.button('æ¸…é™¤ä¼šè¯å†å²', type="primary"):
            st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

prompt = st.chat_input('è¯´ç‚¹ä»€ä¹ˆå§...')


def c():
    with st.chat_message("user"):
        st.markdown(prompt)

    if llm:
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            for response, history in model.stream_chat(tokenizer, prompt, history=st.session_state.messages):
                response_placeholder.markdown(response)
            st.session_state.messages = history
    else:
        with st.chat_message("assistant"):
            st.markdown('æ²¡æœ‰ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹')
        st.session_state.messages.append({'role': 'user', 'content': prompt})
        st.session_state.messages.append(
            {'role': 'assistant', 'content': "æ²¡æœ‰ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹"})

    if voice:
        say(response, voice=tts_voice)

if prompt:

    c()


if micphone:
    a = True
    while a:
        r = sr.Recognizer()
        with sr.Microphone() as source:
            mic_placeholder.markdown('say')
            audio = r.listen(source, timeout=None, phrase_time_limit=10)
            mic_placeholder.markdown('recognition')
            asr_message = r.recognize_whisper(
                audio, model="small", language="chinese")
            mic_placeholder.markdown(asr_message)
            if asr_message:
                prompt = asr_message
                c()                
            if asr_message == 'é€€å‡º':
                a = False
