import streamlit as st
# from libs.models.mscope import get_model
# from libs.asr.speechrecognition import listen
# from libs.tts.edge import say
from libs.info.vsp import get_patient_info
# import speech_recognition as sr

st.set_page_config(
    page_title="ä¹³è…ºå¤–ç§‘è™šæ‹Ÿç—…äºº",
    page_icon="ğŸ‘©",
    layout='wide',
    initial_sidebar_state='expanded',
)


with st.sidebar:
    st.title('ğŸ‘© - BreastVSP -')
    with st.container(border=True):
        llm = st.toggle('å¤§è¯­è¨€æ¨¡å‹')
        model = st.selectbox('è¯­è¨€æ¨¡å‹', ['THUDM/ChatGLM3'])
    with st.container(border=True):
        micphone = st.toggle('éº¦å…‹é£è¾“å…¥')
        asr = st.selectbox('è¯­éŸ³è¯†åˆ«', ['openai/whisper'])
    with st.container(border=True):
        voice = st.toggle('è¯­éŸ³è¾“å‡º')
        tts = st.selectbox('å‘éŸ³æ¨¡å‹', ['rany2/edge-tts'])
        tts_voice = st.selectbox('è¯­éŸ³é€‰æ‹©', ['zh-CN-XiaoxiaoNeural', 'zh-CN-XiaoyiNeural', 'zh-CN-liaoning-XiaobeiNeural',
                                 'zh-CN-shaanxi-XiaoniNeural', 'zh-HK-HiuGaaiNeural', 'zh-HK-HiuMaanNeural', 'zh-TW-HsiaoChenNeural', 'zh-TW-HsiaoYuNeural'])
    with st.container(border=True):
        patient = st.selectbox('æ‚£è€…ä¿¡æ¯', ['random'])

# patient = get_patient_info()


info_col, empty_col, chat_col, right_col = st.columns([1, 1, 3, 1])

with info_col:
    st.image('patient.jpeg')
    if "patient" not in st.session_state:
        st.session_state.patient = get_patient_info()
        patient = st.session_state.patient
    else:
        patient = st.session_state.patient
    st.markdown(f'''
                #### {patient.name}
                ###### å¹´é¾„ï¼š{patient.age}å²
                ###### åœ°å€ï¼š{patient.address}
                ###### ç”µè¯ï¼š{patient.phone}
                ''')
    if st.button('æ›´æ”¹æ‚£è€…', use_container_width=True):
        st.session_state.patient = get_patient_info()
        patient = st.session_state.patient


def chat():

    # tokenizer, model = get_model()
    #     with st.chat_message("assistant"):
    #         message_placeholder = st.empty()
    #         for response, history in model.stream_chat(tokenizer, prompt, history=list(st.session_state.messages)):
    #             message_placeholder.markdown(response)
    return 'USING LLM' if llm else 'NO LLM'


with chat_col:
    chat_holder = st.text_area()
    if "messages" not in st.session_state:
        st.session_state.messages = []
    prompt = st.text_input('è¯´ç‚¹ä»€ä¹ˆå§...', '')
    if prompt:
        st.session_state.messages.append({"role": "user", "content": prompt})
        response = chat()
        st.session_state.messages.append(
            {"role": "assistant", "content": response})
        x = ''
        for message in st.session_state.messages:
            x = x + f"{message['role']}, {message['content']}\n"
        chat_holder.text(x)

    # pro = st.empty()
    # if st.button('listen'):
    #     a = True
    #     while a:
    #         r = sr.Recognizer()
    #         with sr.Microphone() as source:
    #             pro.text('say')
    #             audio = r.listen(source, timeout=None, phrase_time_limit=10)
    #             pro.text('recognition')
    #             message = r.recognize_whisper(audio, model="base", language="chinese")
    #             pro.text(message)
    #             say(message)
    #             if message == 'é€€å‡º':
    #                 a = False
