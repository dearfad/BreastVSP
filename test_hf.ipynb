{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70a4e20b-49e4-4753-91f3-843ef5f2b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95fbf634-1548-449f-954f-e5ae4ae5055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('THUDM/chatglm3-6b', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ae5edd0-8571-4a35-b1ef-0db9b97bedac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:10<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('THUDM/chatglm3-6b', trust_remote_code=True).quantize(4).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fcc70df-3b08-479f-b112-b7f93e2a9f21",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'round_up' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mD:\\Github\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm3-6b\\456aa875cf1f46623006edaa23103774ea9c0eae\\modeling_chatglm.py:1208\u001b[0m, in \u001b[0;36mChatGLMForConditionalGeneration.quantize\u001b[1;34m(self, bits, empty_init, device, **kwargs)\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mquantization_bit \u001b[38;5;241m=\u001b[39m bits\n\u001b[1;32m-> 1208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m quantize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mencoder, bits, empty_init\u001b[38;5;241m=\u001b[39mempty_init, device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m   1209\u001b[0m                                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mD:\\Github\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm3-6b\\456aa875cf1f46623006edaa23103774ea9c0eae\\quantization.py:155\u001b[0m, in \u001b[0;36mquantize\u001b[1;34m(model, weight_bit_width, empty_init, device)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Replace fp16 linear with quantized linear\"\"\"\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 155\u001b[0m     layer\u001b[38;5;241m.\u001b[39mself_attention\u001b[38;5;241m.\u001b[39mquery_key_value \u001b[38;5;241m=\u001b[39m \u001b[43mQuantizedLinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_bit_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_bit_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_key_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_key_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_key_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_key_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mempty_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mempty_init\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     layer\u001b[38;5;241m.\u001b[39mself_attention\u001b[38;5;241m.\u001b[39mdense \u001b[38;5;241m=\u001b[39m QuantizedLinear(\n\u001b[0;32m    164\u001b[0m         weight_bit_width\u001b[38;5;241m=\u001b[39mweight_bit_width,\n\u001b[0;32m    165\u001b[0m         weight\u001b[38;5;241m=\u001b[39mlayer\u001b[38;5;241m.\u001b[39mself_attention\u001b[38;5;241m.\u001b[39mdense\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_device()),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m         empty_init\u001b[38;5;241m=\u001b[39mempty_init\n\u001b[0;32m    170\u001b[0m     )\n\u001b[0;32m    171\u001b[0m     layer\u001b[38;5;241m.\u001b[39mmlp\u001b[38;5;241m.\u001b[39mdense_h_to_4h \u001b[38;5;241m=\u001b[39m QuantizedLinear(\n\u001b[0;32m    172\u001b[0m         weight_bit_width\u001b[38;5;241m=\u001b[39mweight_bit_width,\n\u001b[0;32m    173\u001b[0m         weight\u001b[38;5;241m=\u001b[39mlayer\u001b[38;5;241m.\u001b[39mmlp\u001b[38;5;241m.\u001b[39mdense_h_to_4h\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_device()),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m         empty_init\u001b[38;5;241m=\u001b[39mempty_init\n\u001b[0;32m    178\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Github\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm3-6b\\456aa875cf1f46623006edaa23103774ea9c0eae\\quantization.py:139\u001b[0m, in \u001b[0;36mQuantizedLinear.__init__\u001b[1;34m(self, weight_bit_width, weight, bias, device, dtype, empty_init, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(weight \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_scale[:, \u001b[38;5;28;01mNone\u001b[39;00m])\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint8)\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight_bit_width \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m \u001b[43mcompress_int4_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mto(device), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_scale \u001b[38;5;241m=\u001b[39m Parameter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_scale\u001b[38;5;241m.\u001b[39mto(device), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\Github\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm3-6b\\456aa875cf1f46623006edaa23103774ea9c0eae\\quantization.py:76\u001b[0m, in \u001b[0;36mcompress_int4_weight\u001b[1;34m(weight)\u001b[0m\n\u001b[0;32m     73\u001b[0m stream \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_stream()\n\u001b[0;32m     75\u001b[0m gridDim \u001b[38;5;241m=\u001b[39m (n, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 76\u001b[0m blockDim \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mmin\u001b[39m(\u001b[43mround_up\u001b[49m(m, \u001b[38;5;241m32\u001b[39m), \u001b[38;5;241m1024\u001b[39m), \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     78\u001b[0m kernels\u001b[38;5;241m.\u001b[39mint4WeightCompression(\n\u001b[0;32m     79\u001b[0m     gridDim,\n\u001b[0;32m     80\u001b[0m     blockDim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m     [ctypes\u001b[38;5;241m.\u001b[39mc_void_p(weight\u001b[38;5;241m.\u001b[39mdata_ptr()), ctypes\u001b[38;5;241m.\u001b[39mc_void_p(out\u001b[38;5;241m.\u001b[39mdata_ptr()), ctypes\u001b[38;5;241m.\u001b[39mc_int32(n), ctypes\u001b[38;5;241m.\u001b[39mc_int32(m)],\n\u001b[0;32m     84\u001b[0m )\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mNameError\u001b[0m: name 'round_up' is not defined"
     ]
    }
   ],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc4821d-088e-4ea8-8ebe-c8c908dff529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
